{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further hypothesis testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3. Multiple hypothesis correction and normality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Multiple hypothesis correction\n",
    "\n",
    "Unfortunately, there is a problem with the previous analysis.\n",
    "\n",
    "Recall that the significance level, $\\alpha$, is defined as the probability of incorrectly rejecting $H_0$ when it is actually true (i.e. the probability of a Type I error).\n",
    "\n",
    "When we perform [*multiple related hypothesis tests*](https://en.wikipedia.org/wiki/Multiple_comparisons_problem), we increase the chances of producing such a Type I error.\n",
    "\n",
    "For example, if $\\alpha=0.05$ and we perform 100 tests, we *expect* to generate 5 Type I errors. This can be a serious problem when large numbers of hypothesis tests are carried out simultaneously, for example in screening thousands of genes for association with a disease.\n",
    "\n",
    "We therefore need a strategy to control the rate of Type I errors. A very simple approach is given by the [*Bonferroni correction*](https://en.wikipedia.org/wiki/Bonferroni_correction):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv(\"stars.csv\")\n",
    "type_key = ['Brown Dwarf', 'Red Dwarf', 'White Dwarf', 'Main Sequence', 'Supergiant','Hypergiant']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i) Bonferroni correction\n",
    "\n",
    "When conducting $n$ related hypothesis tests, we reduce the significance level for each test to $\\alpha/n$.\n",
    "\n",
    "The probability of making a Type I error *over the whole set of tests* (known as the *family-wise error rate*, FWER) therefore remains at $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with uncorrected alpha = 5.e-02 :\n",
      "0 Brown Dwarf : p = 2.158e-03 *** REJECT H0 ***\n",
      "1 Red Dwarf : p = 1.742e-02 *** REJECT H0 ***\n",
      "2 White Dwarf : p = 1.430e-01 \n",
      "3 Main Sequence : p = 5.823e-02 \n",
      "4 Supergiant : p = 3.660e-03 *** REJECT H0 ***\n",
      "5 Hypergiant : p = 5.311e-07 *** REJECT H0 ***\n",
      "\n",
      "with Bonferroni correction, alpha/n = 8.333e-03 :\n",
      "0 Brown Dwarf : p = 2.158e-03 *** REJECT H0 ***\n",
      "1 Red Dwarf : p = 1.742e-02 \n",
      "2 White Dwarf : p = 1.430e-01 \n",
      "3 Main Sequence : p = 5.823e-02 \n",
      "4 Supergiant : p = 3.660e-03 *** REJECT H0 ***\n",
      "5 Hypergiant : p = 5.311e-07 *** REJECT H0 ***\n"
     ]
    }
   ],
   "source": [
    "# Bonferroni correction\n",
    "\n",
    "p_values = []\n",
    "alpha = 0.05\n",
    "n = 6\n",
    "\n",
    "for t in range(0,n):\n",
    "    sample = data[data.type == t].temperature.apply(np.log)\n",
    "    p_values.append(stats.shapiro(sample)[1])\n",
    "\n",
    "print(\"with uncorrected alpha =\",np.format_float_scientific(alpha,3),\":\")\n",
    "for i in range(0,n):\n",
    "    result = \"\"\n",
    "    if(p_values[i] < alpha): result = \"*** REJECT H0 ***\"\n",
    "    print(i, type_key[i], \": p =\", np.format_float_scientific(p_values[i],3), result) \n",
    "\n",
    "\n",
    "    \n",
    "print(\"\\nwith Bonferroni correction, alpha/n =\",np.format_float_scientific(alpha/n,3),\":\")\n",
    "for i in range(0,n):\n",
    "    result = \"\"\n",
    "    if(p_values[i] < alpha/n): result = \"*** REJECT H0 ***\"\n",
    "    print(i, type_key[i], \": p =\", np.format_float_scientific(p_values[i],3), result) \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "After correcting for multiple hypothesis testing, the red dwarf p-value is not significant.\n",
    "\n",
    "We should report to Professor Xu that log(temperature) is not normally distributed for the brown dwarf, supergiant and hypergiant types.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii) Benjamin-Hochberg correction\n",
    "\n",
    "The Bonferroni correction is simple to apply, but it may be too conservative when there is a very large numbers of tests, or when the tests are not independent (for example, genes are often related to other genes so are likely to share properties).\n",
    "\n",
    "The [*Benjamini-Hochberg procedure*](https://en.wikipedia.org/wiki/False_discovery_rate#Benjaminiâ€“Hochberg_procedure) is an alternative approach. Instead of controlling the FWER, this method controls the *proportion of the positive tests that are incorrect*, i.e. the proportion of rejected $H_0$'s that are Type I errors. This is known as the *false-discovery rate*, FDR.\n",
    "\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
